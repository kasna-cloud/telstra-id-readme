steps:
- name: 'gcr.io/cloud-builders/git'
  id: Clone Dataflow templates
  args:
  - 'clone'
  - 'https://github.com/GoogleCloudPlatform/DataflowTemplates'
- name: 'gcr.io/cloud-builders/mvn'
  id: mvn build
  entrypoint: /bin/sh
  args:
  - '-c'
  - |
    cd DataflowTemplates && \
    mvn compile exec:java \
    -Dexec.mainClass=com.google.cloud.teleport.templates.KafkaToBigQuery \
    -Dexec.cleanupDaemonThreads=false \
    -Dexec.args="--project=${PROJECT_ID} \
    --stagingLocation=gs://${PROJECT_ID}-storage/pipelines/dataflow/kafka2bq/staging \
    --tempLocation=gs://${PROJECT_ID}-storage/pipelines/dataflow/kafka2bq/temp \
    --templateLocation=gs://${PROJECT_ID}-storage/pipelines/dataflow/kafka2bq/template \
    --subnetwork=https://www.googleapis.com/compute/v1/projects/anz-cde-ic-net-host-b45bde/regions/australia-southeast1/subnetworks/${PROJECT_ID}-subnet \
    --network=anz-cde-ic-net-vpc \
    --runner=DataflowRunner \
    --usePublicIps=true"
- name: 'gcr.io/cloud-builders/gcloud'
  id: create table if does not exist
  entrypoint: /bin/sh
  args:
  - '-c'
  - |
    touch .bigqueryrc && \
    bq mk --table --description "datasynth transactions" \
        ${PROJECT_ID}:${_BQ_DATASET}.${_BQ_TABLE} ${_BQ_TABLE}-schema.json \
        || true
- name: 'gcr.io/cloud-builders/gcloud'
  id: run job
  entrypoint: /bin/sh
  args:
  - '-c'
  - |
    cd DataflowTemplates && \
    gcloud dataflow jobs run kafka-to-bigquery-`date +"%Y%m%d-%H%M%S%z"` \
        --gcs-location=gs://${PROJECT_ID}-storage/pipelines/dataflow/kafka2bq/template \
        --zone=australia-southeast1-a \
        --parameters=bootstrapServers=${_KAFKA_BROKER},inputTopic=${_KAFKA_TOPIC},outputTableSpec=${PROJECT_ID}:${_BQ_DATASET}.${_BQ_TABLE},outputDeadletterTable=${PROJECT_ID}:${_BQ_DATASET}.${_BQ_TABLE}_deadletter
substitutions:
    _KAFKA_BROKER: 'anz-cde-ic-ss-kafka.lab.kasna.internal:9094'
    _KAFKA_TOPIC: 'synth-topic'
    _BQ_DATASET: 'team_dataset'
    _BQ_TABLE: 'transactions'
