# kafka2avro

This example shows how to use Apache Beam and SCIO to read objects from a Kafka
topic, and serialize them encoded as Avro files into Google Cloud Storage. 

* [Kafka2Avro](src/main/scala/com/google/cloud/pso/kafka2avro/Kafka2Avro.scala): reads objects from Kafka, convert them to Avro, and write the
  output to Google Cloud Storage. This is a streaming mode pipeline

## Configuration

Before compiling and generating your package, you need to change some options in
[`src/main/resources/application.conf`](src/main/resources/application.conf):

* `broker`: String with the address of the Kafka brokers.
* `kafka-topic`: The name of the topic where the objects are written to, or read
  from.
* `num-demo-objects`: Number of objects that will be generated by the Object2Kafka
  pipeline, these objects can be read with the Kafka2Avro pipeline to test that
  everything is working as expected.

The configuration file follows [the HOCON format](https://github.com/lightbend/config/blob/master/README.md#using-hocon-the-json-superset).

Here is a sample configuration file with all the options set:

```bash
broker = "1.2.3.4:9092"
kafka-topic = "my_kafka_topic"
num-demo-objects = 500  # comments are allowed in the config file
```

## Pre-requirements

### Build tool

This example is written in Scala and uses SBT as build tool.

You need to have SBT >= 1.0 installed. You can download SBT from https://www.scala-sbt.org/

The Scala version is 2.12.8. If you have the JDK > 1.8 installed, SBT should automatically download the Scala compiler.

## Compile

Run `sbt` in the top sources folder.

Inside sbt, download all the dependencies:

```
sbt:kafka2avro> update
```

and then compile

```
sbt:kafka2avro> compile
```

## Deploy and run

If you have managed to compile the code, you can generate a JAR package to be
deployed on Dataflow, with:

```
sbt:kafka2avro> assembly
```

This will generate a fat JAR file target/scala-2.12/kafka2avro-assembly-0.1.0-SNAPSHOT.jar

### Running the Kafka2Avro pipeline in local

This is a streaming pipeline. It will keep running unless you cancel it. The
default windowing policy is to group messages every 2 minutes, in a fixed
window. To change the policy, please see 
[the function `windowIn` in `Kafka2Avro.scala`](src/main/scala/com/google/cloud/pso/kafka2avro/Kafka2Avro.scala#L62-L72).

Once you have generated the JAR file using the `pack` command inside SBT, you
can now launch the job in Dataflow to populate Kafka with some demo
objects. Using Java 1.8, run the following command. Notice that you have to set
the project id, and a location in a GCS bucket to store the JARs imported by
Dataflow:

```
java -jar target/scala-2.12/kafka2avro-assembly-0.1.0-SNAPSHOT.jar
--project=__ENVSUBST__PROJECT_ID__ --stagingLocation="gs://YOUR_BUCKET/YOUR_STAGING_LOCATION" --runner=DataflowRunner --destBucket=YOUR_DESTINATION_GCP_BUCKET--destPath=YOUR_DESTINATION_PATH
```

Please remember that the machine running the JAR may need to have connectivity
to the Kafka cluster in order to retrieve some metadata, prior to launching the
pipeline in Dataflow.

**Remember that this is a streaming pipeline, it will keep running forever until
you cancel or stop it.**


## Continuous Integration

This example includes [a configuration file for Cloud Build that create a docker image with kafka2avro jar embedded](cloudbuild-package.yaml), 
so you can use it to run the unit tests with every commit done to your repository. 
To use this configuration file:

* Add your sources to a Git repository (either in Bitbucket, Github or Google
  Cloud Source).
* Configure a trigger in Google Cloud Build linked to your Git repository.
* Set the path for the configuration file to [`cloudbuild-package.yaml`](cloudbuild-package.yaml).

The included configuration file will do the following steps:
* Download a cache for Ivy2 from a Google Cloud Storage bucket named
  __ENVSUBST__PROJECT_ID___cache.
* Compile and test the Scala code.
* Generate a package.
* Create docker image kafka2avro 
* pushed the docker image to GCR


Please note that you need to build and include the `scala-sbt` Cloud Builder in
order to use this configuration file.

* Make sure you have the Google Cloud SDK configured with your credentials and
  project
* Download the sources from
  [GoogleCloudPlatform/cloud-builders-community/tree/master/scala-sbt](https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/scala-sbt)
* And then in the `scala-sbt` sources dir, run `gcloud builds submit
  . --config=cloudbuild.yaml` to add the builder to your GCP project. You only
  need to do this once.
