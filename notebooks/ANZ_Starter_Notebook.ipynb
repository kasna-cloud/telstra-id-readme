{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# ANZ Data Tutorial Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This notebook will introduce some of the key ANZ tools available during the training event.The event will have functional shared services which expose simulated ANZ data services and APIs which can be used in the development of products and services. These are GCP and Data services.\n",
    "\n",
    "The ANZ CDE and GCP is being developed so that ANZ teams can come up with novel ways to slice and present transactional data. \n",
    "The CDE is a cloud-based analytical platform which provides the capability to acquire real-time and batch third-party data for data integration, enrichment and storage. \n",
    "For the purposes of the Innovation Challenge a data Synthesizer has been developed to create batch and real-time data.\n",
    "\n",
    "Customer Data Ecosystem (CDE) use cases are:\n",
    "1. Use a data discovery environment that will contain synthetic payments and accounts\n",
    "2. Explore the data and develop novel insights\n",
    "3. Deploy steaming data to Data Factory\n",
    "4. Surface ingested, streamed data into a mock mobile application\n",
    "\n",
    "CDE represents both a \"shared\" or \"master\" project which curates and manages\n",
    "access to data for a Party Data Factory (PDF). A PDF is a \"team\" project in the\n",
    "Innovation challenge. A PDF contains data and services which are specific to \n",
    "a business stream. In the Innovation Challenge CDE data master is called\n",
    "`shared services` and the Party Data Factory (PDF) is called a `team`.\n",
    "\n",
    "The Shared Services project contains a GKE which runs a python program to \n",
    "simulate customer transactions - datasynth. This program generates batch files\n",
    "of transactions and writes to a GKE hosted kafka topic to real-time transactions.\n",
    "\n",
    "Each team project also contains a GKE and a jupyterhub service for data discovery.\n",
    "These projects also contain a kafka and BigQuery service which can be written to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Quickstart (TL;DR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Shared Resources\n",
    "- Kafka Transaction topic: `anz-uc-ss-kafka.lab.kasna.internal:9094, synth-topic`\n",
    "- Kafka Payment topic: `anz-uc-ss-kafka.lab.kasna.internal:9094, payment-topic`\n",
    "- Storage bucket: `gs://anz-uc-ss-storage`\n",
    "- BigQuery transaction table: `anz-uc-ss.sample.transactions`\n",
    "- BigQuery customers table: `anz-uc-ss.sample.customers`\n",
    "- BigQuery accounts table: `anz-uc-ss.sample.accounts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Team Resources\n",
    "- [Juypterhub login](https://anz-uc-team-1.lab.kasna.cloud)\n",
    "- Storage: `gs://anz-uc-team-1-storage/`\n",
    "- Team Repo: `gcloud source repos clone  --project=anz-uc-team-1`\n",
    "- BigQuery dataset: `anz-uc-team-1.team_dataset`\n",
    "- Kafka for team: `anz-uc-team-1.lab.kasna.internal:9094`\n",
    "- [Airflow](https://anz-uc-team-1-airflow.lab.kasna.cloud)\n",
    "- GKE creds: `gcloud container clusters get-credentials --project=\"anz-uc-team-1\" --zone=\"australia-southeast1-a\" \"anz-uc-team-1-gke\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This is a tutorial on how to use datalab for the ANZ Innovation Challenge. Skip this section if you are already familiar with Jupyter. Detailed examples on how to use Jupyter and Python can be found online. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To run python code, either press run on the toolbar or press shift+enter while a cell is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are some datasources provided in the Innovation Challenge, including Bigquery and Kafka. The following variables are provided for further use in the notebook. These are SHARED SERVICES resources which all teams will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "KAFKA_HOST = 'anz-uc-ss-kafka.lab.kasna.internal'\n",
    "KAFKA_PORT = '9094'\n",
    "KAFKA = KAFKA_HOST + \":\" + KAFKA_PORT\n",
    "KAFKA_PUSH_TOPIC = 'synth-topic'\n",
    "KAFKA_SUBSCRIBE_TOPIC = 'payment-topic'\n",
    "BQ_PROJECT = 'anz-uc-ss'\n",
    "BQ_DATASET = 'sample'\n",
    "BQ_TABLE = 'transacions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Your team will use resource which you can read and write to and are only for your team. These resources are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = os.environ.get('PROJECT_ID')\n",
    "USER = os.environ.get('JUPYTERHUB_USER')\n",
    "print(\"Your username is: \" + USER)\n",
    "print(\"Your Project ID: \" + PROJECT_ID)\n",
    "TEAM_KAFKA_HOST = PROJECT_ID + '-kafka.lab.kasna.internal'\n",
    "TEAM_KAFKA_PORT = '9094'\n",
    "TEAM_KAFKA = TEAM_KAFKA_HOST + \":\" + KAFKA_PORT\n",
    "TEAM_KAFKA_PUSH_TOPIC = 'synth-topic'\n",
    "TEAM_KAFKA_SUBSCRIBE_TOPIC = 'payment-topic'\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = 'team_dataset'\n",
    "REPO = \"https://source.cloud.google.com/\" + PROJECT_ID + \"/\" + PROJECT_ID + \"-team\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Google Cloud Platform's BigQery enables interactive analysis of massive datasets. BigQuery uses the SQL language to query datasets and tables of data.\n",
    "\n",
    "For the Innovation Challege there is an instance of BigQuery within the Shared Service project and within your team's project. The Shared BigQuery contains a read-only copy of the transactions generated by Datasynth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Explore Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The following code takes a SQL statement and queries BigQuery. The result is then saved to a Pandas Dataframe called total_births, located after the %%bigquery statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bigquery total_births\n",
    "SELECT\n",
    "    source_year AS year,\n",
    "    COUNT(is_male) AS birth_count\n",
    "FROM `bigquery-public-data.samples.natality`\n",
    "GROUP BY year\n",
    "ORDER BY year DESC\n",
    "LIMIT 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "total_births.plot(x='year',y='birth_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Explore Datasynth transactions\n",
    "\n",
    "Now we'll use the Sample data, and see if we can gain any insights from it. \n",
    "\n",
    "We can get the base data from the shared service project bigquery, and even do some simple models via BigqueryMl on 90% of the dataset.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE MODEL `team_dataset.simple_model`\n",
    "OPTIONS(model_type='linear_reg',input_label_cols=['txn_amount_int']) AS\n",
    "SELECT\n",
    "CAST(txn_amount AS NUMERIC) as txn_amount_int,\n",
    "CAST(txn_available_balance AS NUMERIC) as balance\n",
    "FROM `anz-uc-team-1.team_dataset.transactions*`\n",
    "WHERE MOD(ABS(FARM_FINGERPRINT(txn_id)),10) != 0 AND txn_date BETWEEN '2019-07-03' AND '2019-07-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "SELECT * FROM ML.EVALUATE(MODEL `team_dataset.simple_model`,\n",
    "(SELECT\n",
    "CAST(txn_amount AS NUMERIC) as txn_amount_int,\n",
    "CAST(txn_available_balance AS NUMERIC) as balance\n",
    "FROM `anz-uc-team-1.team_dataset.transactions*`\n",
    "WHERE MOD(ABS(FARM_FINGERPRINT(txn_id)),10) = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bigquery predictions\n",
    "SELECT * FROM ML.PREDICT(MODEL `team_dataset.simple_model`,\n",
    "(SELECT\n",
    "CAST(txn_amount AS NUMERIC) as txn_amount_int,\n",
    "CAST(txn_available_balance AS NUMERIC) as balance\n",
    "FROM `anz-uc-team-1.team_dataset.transactions*`\n",
    "WHERE txn_date BETWEEN '2019-07-03' AND '2019-07-10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "h =predictions.plot('balance',y=['txn_amount_int','predicted_txn_amount_int'],style=['bo','r-'],ms=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Can you see any correlation between how much people have and how much people spend? Can you think of a reason for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Kafka is a stream processing platform used for high-volume publish and subscribe. CDE uses kafka to publish data to Party Data Factories. In the ANZ Innovation Challenge kafka is used to transfer data messages between the Datasynth service and the teams. The examples below walk through some of this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "!pip install kafka-python\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from kafka import KafkaConsumer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Read from Kafka synth-topic\n",
    "\n",
    "Quick python to read off of the Shared Kafka synth-topic queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import time\n",
    "print(\"Pull from host, \" + KAFKA + \" - \" + KAFKA_PUSH_TOPIC)\n",
    "consumer = KafkaConsumer(KAFKA_PUSH_TOPIC,auto_offset_reset='smallest',bootstrap_servers=[KAFKA])\n",
    "num_records_to_show=3\n",
    "i =0\n",
    "for message in consumer:\n",
    "    datalist = message.value.strip().decode('utf-8').split(',')\n",
    "    print(\"===== Message \" + str(i) + \" =====\")\n",
    "    i=i+1\n",
    "    for x in range(len(datalist)):\n",
    "        print(datalist[x])\n",
    "    if i==num_records_to_show:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Plot kafka transactions\n",
    "\n",
    "We can also add interactively updating plots in Jupyter to use real time Kafka data about current transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import time, json\n",
    "print(\"Pull from host, \" + KAFKA + \" - \" + KAFKA_PUSH_TOPIC)\n",
    "consumer = KafkaConsumer(KAFKA_PUSH_TOPIC,auto_offset_reset='earliest',bootstrap_servers=[KAFKA])\n",
    "num_records_to_show=300\n",
    "i =0\n",
    "transaction_amounts = []\n",
    "for message in consumer:\n",
    "  data = json.loads(message.value.strip().decode('utf-8'))\n",
    "  transaction_amounts.append(float(data['txn_amount']))\n",
    "  i+=1\n",
    "  if i % 10==0:\n",
    "    %matplotlib inline\n",
    "    plt.plot(range(i),transaction_amounts,'r.',markersize=4)\n",
    "    plt.ylim(0,num_records_to_show)\n",
    "    plt.xlim(0,300)\n",
    "    plt.gcf().set_size_inches(30,6)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.gcf().show()\n",
    "  if i% num_records_to_show==0:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Team Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A team git repo has been created which provides some tools and examples for use in development. This repo is located in Google Cloud Source Repository within your project and is named: {{ PROJECT_ID }}-team\n",
    "\n",
    "There are several methods to clone this repo and begin using the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Clone using Cloud Source Repository\n",
    "\n",
    "To clone using CSR, navigate to the Cloud Source Repository and follow the instructions for cloning using:\n",
    "- VS Code\n",
    "- Gcloud SDK\n",
    "- Manually using GCP credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"Your team repo is here: \" + REPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Use the jupyterlab Repo\n",
    "\n",
    "You can interact with the team repo through the file navigator and git extenstion. To find the repo select the folder icon in the top left and select `team-repo`.\n",
    "\n",
    "To pull/push and commit changes to the repo, you can use the git extension found in the left hand navigation. Use this tool to pull and push changes, stage files and commit. Ensure to push changes after commiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Clone in Theia IDE\n",
    "\n",
    "Eclipse Theia is a web-based IDE modeled on VS Code. An instance of Theia is available in this jupyterhub environment and can use the team repo is already availble for use. To develop in this IDE do the following:\n",
    "1. Navigate to the Launcher - if this is not open you can create a new Launcher using `File -> New Launcher`\n",
    "2. Select the Theia icon\n",
    "3. Once Theia is loaded select `File -> Open Workspace`\n",
    "4. Navigate to `team-repo` and select open.\n",
    "\n",
    "Save the workspace to use during your next session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Airflow UI Access\n",
    "\n",
    "Every member of a team has access to a pre-created airflow instance and can log into the airflow web-ui located at https://${PROJECT_ID}-airflow.lab.kasna.cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"Your airflow instance is here:  https://\" + PROJECT_ID + \"-airflow.lab.kasna.cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The website is authenticated by google oauth credentials and uses should login using their kasna google credentials.\n",
    "\n",
    "![Airflow UI](team-repo/airflow/images/airflow-ui.png)\n",
    "\n",
    "### Airflow Jobs\n",
    "\n",
    "Once users login to the web-ui there are some pre-defined sample jobs which are defined using job templates. Airflow has been pre-configured to periodically load the jobs(dags) uploaded to a google cloud storage bucket(gs://${PROJECT_ID}-storage). \n",
    "The [cloudbuild.yaml](./cloudbuild.yaml) defines a cloud build job to upload the src directory containing job definitions to the storage bucket.\n",
    "\n",
    "![Airflow Storage](team-repo/airflow/images/dags-storage.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Yaml to DAGs \n",
    "\n",
    "By Convention airflow jobs are written in python however to simulate CDE, we will be utilizing an existing library to dynamically create Directed acyclic graphs (DAGs) from yaml files.\n",
    "\n",
    "For example, the yaml file below is a simple job creating two bash operator tasks, scheduled to run hourly, and with task_2 dependant on task_1.\n",
    "\n",
    "```\n",
    "1_example_dag:\n",
    "  default_args:\n",
    "    start_date: 2019-09-04\n",
    "    timezone: 'Australia/Melbourne' \n",
    "  schedule_interval: '0 * * * *'\n",
    "  description: 'this is example dag'\n",
    "  tasks:\n",
    "    task_1:\n",
    "      operator: airflow.operators.bash_operator.BashOperator\n",
    "      bash_command: 'echo 1'\n",
    "    task_2:\n",
    "      operator: airflow.operators.bash_operator.BashOperator\n",
    "      bash_command: 'echo 2'\n",
    "      dependencies: [task_1]\n",
    "```\n",
    "\n",
    "The dag.py file in the src will render dags from .yaml files in the templates directory. One can also customize timezone, schedule_interval and other airflow job options.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Sample Jobs\n",
    "\n",
    "These are few sample jobs to understand airflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 1_example_dag\n",
    "\n",
    "This is a simple job with three `bash operator` tasks to demonstrate airflow as a scheduler.\n",
    "\n",
    "template:\n",
    "[JOB1](./team-repo/airflow/src/templates/job1.yaml) \n",
    "\n",
    "1) Run the job. Click on the button as show in the image below.\n",
    "\n",
    "![JOB1](team-repo/airflow/images/job1-run.png \"JOB 1\")\n",
    "\n",
    "2) View the graph . Click on the link of the job and click on Graph View\n",
    "\n",
    "![JOB1](team-repo/airflow/images/job1.png \"JOB 1\")\n",
    "\n",
    "3) View the Task Log\n",
    "\n",
    "Click on any node in the graph and select View Log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 2_bq_dataset_creation\n",
    "\n",
    "This job utilizes the `gcloud operator` to create a BigQuery dataset and a table.\n",
    "\n",
    "template:\n",
    "[JOB2](./team-repo/airflow/src/templates/job2.yaml)\n",
    "\n",
    "1) Run the job just as in Job1\n",
    "\n",
    "2) View the graph. Click on the link of the job and click on Graph View.\n",
    "\n",
    "![JOB2](team-repo/airflow/images/job2.png \"JOB 2\")\n",
    "\n",
    "3) View the Task Log\n",
    "\n",
    "Click on any node in the graph and select `View Log`.\n",
    "\n",
    "4) Check Output\n",
    "\n",
    "After the job is successful we should be able to see a dataset and a table in BigQuery console.\n",
    "\n",
    "![JOB2 OUTPUT](team-repo/airflow/images/job2_output.png \"JOB 2 OUTPUT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3_bq_examples\n",
    "\n",
    "In this job we will utilize the above created dataset and table to explore more tasks using `gcloud operators`. we will perform four task in this job. \n",
    "\n",
    "The first task will run a sql query to query a public dataset and create a dump the data into the table we created in the earlier job. we run the sql query from a file uploaded to the cloud storage from the src/sql directory.\n",
    ".\n",
    "The second task will transform the BigQuery table from first task to a cloud storage bucket in avro format.\n",
    "\n",
    "The third task will load the data from second task from cloud storage to another BigQuery table using auto schema.\n",
    "\n",
    "The fourth task will load the data from second task from cloud storage to another BigQuery table using the schema provided loaded into the cloud storage which is uploaded from the src/schemas folder\n",
    "\n",
    "template:\n",
    "[JOB3](./team-repo/src/templates/job3.yaml)\n",
    "\n",
    "1) Run the job. Similar as Job1\n",
    "\n",
    "2) View the graph. Click on the link of the job and click on `Graph View`.\n",
    "\n",
    "![JOB3](team-repo/airflow/images/job3.png \"JOB 3\")\n",
    "\n",
    "3) View the Task Log\n",
    "\n",
    "Click on any node in the graph and select View Log.\n",
    "\n",
    "4) Check Output\n",
    "\n",
    "After the job is successful we should be able to see two additional BigQuery tables loaded from the avro data in cloud storage both with schema and auto schema modes.\n",
    "\n",
    "![JOB3 OUTPUT](team-repo/airflow/images/job3_output.png \"JOB 3 OUTPUT\")\n",
    "![JOB3 OUTPUT](team-repo/airflow/images/job3_output2.png \"JOB 3 OUTPUT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 4_dataflow_job_kubernetes\n",
    "\n",
    "Airflow GCP operators provide different ways to orchestrate jobs within GCP. The `KubernetesPodOperator` can be used in place of the java or other operators to run generic jobs on Kubernetes. For this example we will use the GKE Pod Operator within your GKE cluster to extend execute java classes.\n",
    "\n",
    "Before we can run this job we must build the dataflow job from the `dataflow` directory. This will create a container image with the dataflow job artifacts. \n",
    "\n",
    "From a terminal and the the root directory of this repo, run the command below. This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit team-repo/dataflow/kafka2avro/. --config=team-repo/dataflow/kafka2avro/cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Once the image creation is completed, return to the airflow UI.\n",
    "\n",
    "template:\n",
    "[JOB4](./team-repo/src/templates/job4.yaml)\n",
    "\n",
    "1) Run the job. Similar as Job1\n",
    "\n",
    "2) View the graph . Click on the link of the job and click on Graph View\n",
    "\n",
    "![JOB4](team-repo/airflow/images/job4.png \"JOB 4\")\n",
    "\n",
    "3) View the Task Log\n",
    "\n",
    "Click on any node in the graph and select View Log\n",
    "\n",
    "4) Check Output\n",
    "After the job is submitted we should be able to see the pod task in GKE and the dataflow job created by the pod.\n",
    "\n",
    "![JOB4 OUTPUT](team-repo/airflow/images/job4_output.png \"JOB 4 OUTPUT\")\n",
    "![JOB4 OUTPUT](team-repo/airflow/images/job4_output2.png \"JOB 4 OUTPUT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Sample App to GKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Sample Application\n",
    "\n",
    "This folder has a sample application with a server side api and a client side frondend deployed into gke environment of the team\n",
    "\n",
    "## Server Side Spec\n",
    "\n",
    "Python Flask application\n",
    "\n",
    "Postgres db using CloudSQL via cloudsql proxy\n",
    "\n",
    "GKE Deployment running unicorn app to expose flask api\n",
    "\n",
    "## Client Side Spec\n",
    "\n",
    "Vue Js application\n",
    "\n",
    "GKE deployment with nginx \n",
    "\n",
    "## Deployment Steps\n",
    "\n",
    "To Deploy we first need to create a postgres db and set some initial configuration such as create db, setting kubernetes secret in the namespace for cloudsql proxy and setting db username, password in\n",
    " kubernetes as secrets\n",
    "\n",
    "Please set the enviroment variables:\n",
    "\n",
    "```\n",
    "PROJECT_ID -> project id of your gcp \n",
    "HOST_PROJECT_NAME -> project id of your gcp host project to create cloudsql in sharedvpc\n",
    "NETWORK_NAME -> Shared VPC name of your host project\n",
    "POSTGRES_PASSWORD -> postgress password to use to connect to db\n",
    "```\n",
    "\n",
    "Run the following script to create the db and secrets in kubernetes\n",
    "\n",
    "\n",
    "```cloudsql.sh```\n",
    "\n",
    "\n",
    "After the postgres db is created. We will need to seed data into the database for our api to expose\n",
    "\n",
    "running the following cloudbuild step will create migration job in kubernetes to seed data into postgres\n",
    "\n",
    "```gcloud builds submit . --config=cloudbuild-db-migration.yaml```\n",
    "\n",
    "Then we can deploy the api & frontend into gke using the following step \n",
    "\n",
    "```gcloud builds submit . --config=cloudbuild.yaml```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Datasynth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## About\n",
    "### Input Files\n",
    "\n",
    "The application uses several input files made by the team for the purposes of lookup:\n",
    "\n",
    "- `long_lat.json` contains a list of latitudes and longitudes used to randomly assign to customers.\n",
    "- `merchants_lat_long.json` contains a list of merchants GUIDs for each longitude and latitude location.\n",
    "- `transactions_per_hour.json` contains a list of transactions per hour for each hour of the year.\n",
    "- `merchant_details.json` contains a list of merchants (matches merchants_lat_long.json) with some detailed info: name, suburb, state, etc\n",
    "- `recurring_merchants.json` contains a list of payees for recurring bill payments with such details as name, bpay biller code\n",
    "- `cust_prod.json` contsains a list of subproduct codes for age between 18 to 68\n",
    "\n",
    "### Customer, Account, and Attributes\n",
    "\n",
    "Customers, Accounts, Customer to Account relationships, and Transaction specific Attributes will be generated.\n",
    "\n",
    "This function will create the following files:\n",
    "\n",
    "- `customers.json`\n",
    "- `accounts.json`\n",
    "- `customer_accounts.json`\n",
    "- `attributes.json`\n",
    "- `reccuring_payments.db`\n",
    "\n",
    "### Cache\n",
    "\n",
    "Customer available balances and transaction queue which are stored in-memory are dumped on disk into app/temp directory with the following frequency:\n",
    "- every minute for kafka and stream\n",
    "- every day in date range for batch\n",
    "\n",
    "## Transactions Stream\n",
    "\n",
    "Transactions will be produced at 1 second intervals and sent to `stdout` until the script is stopped.\n",
    "\n",
    "## Transactions Kafka\n",
    "\n",
    "Transactions will be produced at 1 second intervals and sent to kafka topic until the script is stopped.\n",
    "\n",
    "## Transactions Batch\n",
    "\n",
    "Transactions will be produced between 2 provided dates. A file per date will be created in the output directory.\n",
    "\n",
    "This function will create the following files (example):\n",
    "\n",
    "- `transactions_2018-01-01.json`\n",
    "- `transactions_2018-01-02.json`\n",
    "\n",
    "## Test\n",
    "A test script for connecting to the datasynth stream is available in the `team-repo`. You can test the kafka connection using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "!team-repo/kafka/test.sh consumer synth-topic anz-cde-ic-ss --max-messages=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Payment Service Platform\n",
    "\n",
    "Innovation Challenge Architecture \n",
    "Description: \n",
    "\n",
    "Payments Service Platform(PSP) API is meant for performing transactions with the accounts supplied. This involves sending the JSON payload over a POST call to the API which does the required validation of incoming message and once successful writes the payload to a Kafka topic. \n",
    "\n",
    "### PSP Service URLs \n",
    "\n",
    "TODO\n",
    "`http://${PROJECT_ID}-pspapi.lab.kasna.cloud/psp/v1/submitPayment`\n",
    "\n",
    "## Payload Validations: \n",
    "1. Date not in future \n",
    "2. Account is valid \n",
    "3. Customer related to account is active\n",
    "4. Account has sufficient balance \n",
    "\n",
    "## PSP Service Contract \n",
    "\n",
    "### Input Message \n",
    "\n",
    "Sample Payload Request : SamplePayloadRequest.json \n",
    "\n",
    "### PSP API Responses: \n",
    "\n",
    "- Success \n",
    "```\n",
    "{ \n",
    "    \"reqId\": \"<<REQID>>\", \n",
    "    \"response\": \"PAYMENT_SUCCESS\" \n",
    "}\n",
    "```\n",
    "    \n",
    "- Future Date Error\n",
    "```\n",
    "{\n",
    "    \"reqId\": \"<<REQID>>\",\n",
    "    \"response\": \"PAYMENT_FAILED - Future date not allowed\"\n",
    "}\n",
    "```\n",
    "- Amount exceeds available balance: \n",
    "```\n",
    "{\n",
    "    \"reqId\": \"<<REQID>>\", \n",
    "    \"response\": \"PAYMENT_FAILED - Insufficient Amount\" \n",
    "}\n",
    "```\n",
    "\n",
    "- Account not valid: \n",
    "```\n",
    "{\n",
    "    \"reqId\": \"<<REQID>>\", \n",
    "    \"response\": \"PAYMENT_FAILED - Account ID is not Valid \"\n",
    "}\n",
    "```\n",
    "\n",
    "- Customer Inactive: \n",
    "```\n",
    "{\n",
    "    \"reqId\": \"<<REQID>>\", \n",
    "    \"response\": \"PAYMENT_FAILED - Customer not present or inactive \"\n",
    "}\n",
    "```\n",
    "\n",
    "- Global Exception: \n",
    "```\n",
    "{\n",
    "    \"reqId\": \"<REQID>>\", \n",
    "    \"response\": \"PAYMENT_FAILED - General Data Error\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Kafka Topic Details: \n",
    "- Host : ${PROJECT_ID}-kafka.lab.kasna.internal\n",
    "- Port : 9094 \n",
    "- Topic Name : payment-topic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Theia IDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Theia IDE\n",
    "\n",
    "Eclipse Theia is a web-based IDE modeled on VS Code. An instance of Theia is available in this jupyterhub environment and can use the team repo is already availble for use. To develop in this IDE do the following:\n",
    "1. Navigate to the Launcher - if this is not open you can create a new Launcher using `File -> New Launcher`\n",
    "2. Select the Theia icon\n",
    "\n",
    "![Theia UI](team-repo/notebooks/images/open_theia.gif)\n",
    "\n",
    "3. Once Theia is loaded select `File -> Open Workspace`\n",
    "4. Navigate to `team-repo` and select open.\n",
    "\n",
    "![Team Repo](team-repo/notebooks/images/workspace.gif)\n",
    "\n",
    "Save the workspace to use during your next session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# External DNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "You GKE (kubernetes) environment has the ability to update DNS addresses for both internal and external names. The domain names which will work are:\n",
    "- `.lab.kasna.internal` Internal domain, used only for 10.x.x.x addresses\n",
    "- `.lab.kasna.cloud` External domain, can be any external subdomain.\n",
    "\n",
    "The GKE cluster uses external-dns service: https://github.com/kubernetes-incubator/external-dns\n",
    "\n",
    "To add either domains, update you services manifest which is used in Kubernetes to include the following annotation:\n",
    "```\n",
    "metadata:\n",
    "        annotations:\n",
    "            external-dns.alpha.kubernetes.io/hostname: myservice.lab.kasna.internal.\n",
    "```\n",
    "For more information check the external-dns doco in the link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Data Loss Prevention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The account numbers in the transactions in bigquery team dataset tables are currently tokenized using google cloud Data Loss Prevention\n",
    "In this section we will query few transaction rows and call the dlp api to reidentify tokenized data\n",
    "We can query the current DLP deidentification template by making an api request\n",
    "First lets query BQ for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%bigquery` not found.\n"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "acc_number\n",
    "FROM `anz-uc-team-1.team_dataset.transactions*`\n",
    "WHERE MOD(ABS(FARM_FINGERPRINT(txn_id)),10) != 0 AND txn_date BETWEEN '2019-07-02' AND '2019-07-03'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the above account numbers in the following payload to query dlp api for actual account numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Credentialed Accounts\n",
      "ACTIVE  ACCOUNT\n",
      "*       jupyter-sa@anz-uc-team-1.iam.gserviceaccount.com\n",
      "{\n",
      "  \"item\": {\n",
      "    \"table\": {\n",
      "      \"headers\": [\n",
      "        {\n",
      "          \"name\": \"acc_number\"\n",
      "        }\n",
      "      ],\n",
      "      \"rows\": [\n",
      "        {\n",
      "          \"values\": [\n",
      "            {\n",
      "              \"stringValue\": \"ACC-421950600\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"values\": [\n",
      "            {\n",
      "              \"stringValue\": \"ACC-2494704352\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"values\": [\n",
      "            {\n",
      "              \"stringValue\": \"ACC-2640076321\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"overview\": {\n",
      "    \"transformedBytes\": \"41\",\n",
      "    \"transformationSummaries\": [\n",
      "      {\n",
      "        \"field\": {\n",
      "          \"name\": \"acc_number\"\n",
      "        },\n",
      "        \"results\": [\n",
      "          {\n",
      "            \"count\": \"3\",\n",
      "            \"code\": \"SUCCESS\"\n",
      "          }\n",
      "        ],\n",
      "        \"fieldTransformations\": [\n",
      "          {\n",
      "            \"fields\": [\n",
      "              {\n",
      "                \"name\": \"acc_number\"\n",
      "              }\n",
      "            ],\n",
      "            \"primitiveTransformation\": {\n",
      "              \"cryptoReplaceFfxFpeConfig\": {\n",
      "                \"cryptoKey\": {\n",
      "                  \"kmsWrapped\": {\n",
      "                    \"wrappedKey\": \"CiQAWtiaI8FkG4exIuNZIhivv+fmr/fVU6qADcAidqutrMVgm+8SSADNHj8e+YISjf0IX5h0YSTxaC1D/GHXG/wVl67fjhjtZ8mS8jLMUV6MjvJuexkSEnWkE9ONYl4QnLWuy2JW7MiWQx3regxvJg==\",\n",
      "                    \"cryptoKeyName\": \"projects/anz-uc-team-1/locations/global/keyRings/anz-uc-team-1-dlp-key/cryptoKeys/anz-uc-team-1-dlp-key\"\n",
      "                  }\n",
      "                },\n",
      "                \"customAlphabet\": \"AC-1234567890\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"transformedBytes\": \"41\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "To set the active account, run:\n",
      "    $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2902    0  1621  100  1281    772    610  0:00:02  0:00:02 --:--:--  1383\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud auth list\n",
    "curl -H \"Authorization: Bearer $(gcloud auth application-default print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-X POST https://dlp.googleapis.com/v2/projects/anz-uc-team-1/content:reidentify \\\n",
    "-d @- << EOF\n",
    "{\n",
    "  \"reidentifyConfig\":{\n",
    "    \"recordTransformations\":{\n",
    "      \"fieldTransformations\":[\n",
    "        {\n",
    "          \"primitiveTransformation\":{\n",
    "            \"cryptoReplaceFfxFpeConfig\":{\n",
    "              \"cryptoKey\":{\n",
    "                    \"kmsWrapped\": {\n",
    "                      \"wrappedKey\": \"CiQAWtiaI8FkG4exIuNZIhivv+fmr/fVU6qADcAidqutrMVgm+8SSADNHj8e+YISjf0IX5h0YSTxaC1D/GHXG/wVl67fjhjtZ8mS8jLMUV6MjvJuexkSEnWkE9ONYl4QnLWuy2JW7MiWQx3regxvJg==\",\n",
    "                      \"cryptoKeyName\": \"projects/anz-uc-team-1/locations/global/keyRings/anz-uc-team-1-dlp-key/cryptoKeys/anz-uc-team-1-dlp-key\"\n",
    "                    }\n",
    "              },\n",
    "              \"customAlphabet\": \"AC-1234567890\"\n",
    "            }\n",
    "          },\n",
    "          \"fields\":[\n",
    "            {\n",
    "              \"name\":\"acc_number\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"item\":{\n",
    "    \"table\":{\n",
    "      \"headers\":[\n",
    "        {\n",
    "          \"name\":\"acc_number\"\n",
    "        }\n",
    "      ],\n",
    "      \"rows\":[\n",
    "        {\n",
    "          \"values\":[\n",
    "            {\n",
    "              \"stringValue\":\"9C9A-8A14C-61\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"values\":[\n",
    "            {\n",
    "              \"stringValue\":\"73057C777-1-91\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"values\":[\n",
    "            {\n",
    "              \"stringValue\":\"4841249CA33652\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        \n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
