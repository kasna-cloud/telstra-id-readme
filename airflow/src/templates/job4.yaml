4_dataflow_job_kubernetes:
  default_args:
    owner: ${PROJECT_ID}
    start_date: 2019-08-10
    catchup: 'False'
    timezone: 'Australia/Melbourne'
  schedule_interval:
  description: 'This is a dataflow job using gke pod operator'
  tasks:
    dataflow_from_gke_pod:
      operator: airflow.contrib.operators.gcp_container_operator.GKEPodOperator
      image: gcr.io/${PROJECT_ID}/kafka2avro
      namespace: default
      project_id: ${PROJECT_ID}
      location: ${LOCATION}
      cluster_name: ${PROJECT_ID}-gke
      arguments:
        - '--project=${PROJECT_ID}'
        - '--stagingLocation=gs://${PROJECT_ID}-storage/pipelines/dataflow/kafka2avro/staging'
        - '--runner=DataflowRunner'
        - '--zone=australia-southeast1-a'
        - '--usePublicIps=false'
        - '--network=anz-cde-ic-net-vpc'
        - '--destBucket=${PROJECT_ID}-storage'
        - '--destPath=pipelines/dataflow/kafka2avro/output'
        - '--subnetwork=https://www.googleapis.com/compute/v1/projects/anz-cde-ic-net-host-b45bde/regions/australia-southeast1/subnetworks/${PROJECT_ID}-subnet'
      labels:
        task: kafka2avro
      name: kafka-pipeline
      task_id: kafka-pipeline
      get_logs: True
      execution_timeout:
      image_pull_policy: Always
